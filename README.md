# IMDb Reviews Capstone Project

## Objective

IMDb provides a number of datasets in order to analyse titles and their ratings, however, this dataset is insufficient for analysing ratings *over time*. Indeed, IMDb only provides the "weighted average of all the individual user ratings" but not the individual ratings provided by users as well as when the ratings were provided.

By combining the datasets provided IMDb with another dataset consisting of a large number of reviews including the rating given and when it was given, it will be possible to answer questions like:

* Do reviews tend to get more positive/negative over time?
* Are there differences in ratings between reviews left during the week and the ones left during the weekend?
* etc.

## Datasets

* **IMDb reviews**: A large sample of 5.5M reviews left by users of IMDb on movies, TV shows, etc. This data is available [here](https://www.kaggle.com/ebiswas/imdb-review-dataset) in the form of 6 JSON files of approx. 1M reviews each. Only a subset of these reviews (approx. 1.6M) is used in this project.
* **IMDb datasets**: Various datasets on titles stored on IMBd. This data is available [here](https://www.imdb.com/interfaces/) in the form of several compressed tab-separated files.

## Resulting schema

### Fact table

`reviews`

* `review_id`: the review ID generated by IMBd
* `title_id`: the movie ID generated by IMBd
* `review_date`: when the review was posted
* `rating`: the rating given by the reviewer (out of 10)
* `review_text`: the justification of the rating.

### Dimension tables

`titles`: Information about the show reviewed.

* `title_id`, `title`, `title_type`, `year`, `length`, `genres`

`principals`: Information about the principal cast/crew for titles

* `title_id`, `ordering`, `name`, `category`

`time`: the date of all reviews, broken down into specific units

* `review_date`, `day`, `week`, `month`, `year`, `weekday`

## Choice of technology

I have decided to use Redshift for this project as it would adapt and scale well in case:

- The volum of data was multiplied by 100: The nice thing about Redshift clusters is that they can scale up and down easily, in terms of storage capacity as well as in terms of computing power.
- The pipelines would be run on a daily basis by 7 am every day: The pipeline could be orchestrated in Airflow as Airflow can connect well with Redshift clusters with custom hooks (either the Redshift hook or the Postgres one, which is also compatible with Redshift).
- The database needed to be accessed by 100+ people: Amazon Redshift workload management (WLM) can be customised to increase the number of concurrent queries and prioritise certain user groups and/or queries over others. More details can be found [here](https://docs.aws.amazon.com/redshift/latest/dg/c_workload_mngmt_classification.html).

As the volume of data is fixed and no new data is collected as of now, a periodic job for handling new data is not required today. Thus, the data is loaded and transformed into Redshift using plain Python scripts executing SQL queries against the database for:

* Creating the destination tables
* Copying the data over from S3 to Redshift
* Transforming and inserting the final version of the data also into Redshift, in the schema described above.

## Files description

1. `requirements.txt` lists all packages used in the scripts of this repository.
2. `split-json.py` was used to split the origin JSON files containing 1M reviews each into smaller JSON files containing only 2000 reviews each as Redshift COPY command does not accept JSON files bigger than 4MB. 
3. `create-aws-resources.ipynb` creates (and deletes) the required resources on AWS (IAM role, EC2 security group and Redshift cluster) for running the scripts below. Since the data warehouse is created only for this project, we want to keep the cluster running only while we are working on it and delete it when we are done in order not to incur unnecessary costs. 
4. `sql_queries.py` contains all SQL queries used in this project for creating and droping tables as well as for inserting data into them.
5. `create_tables.py` drops and creates the relevant tables in the data warehouse. This script is used to reset tables before running the ETL script.
6. `etl.py` reads and processes the datasets from S3 into staging tables in the data warehouse and, in turn, from the staging tables into analytics tables (see above star schema) within the data warehouse.
7. `test.ipynb` is for performing several test to verify that the data was loaded as expected.

## How to run this project

1. Make sure you have a configuration file `dwh.config` ready with your AWS credentials and the desired configuration of your cluster. Mine looks like the below:

   ```
   [AWS]
   key = paste_your_key_here
   secret = paste_your_secret_key_here
   
   [DWH]
   dwh_cluster_type = multi-node
   dwh_num_nodes = 4
   dwh_node_type = dc2.large
   dwh_cluster_identifier = dwhCluster
   host = to_be_filled_after_cluster_is_created
   dwh_db_name = dwh
   dwh_db_user = dwhuser
   dwh_db_password = pick_a_password_here
   dwh_port = 5439
   
   [IAM_ROLE]
   dwh_iam_role_name = dwhRole
   arn = arn:aws:iam::041306370667:role/dwhRole
   
   [EC2]
   dwh_sec_group_name = redshift-sg
   
   [S3]
   imdb_reviews = 's3://imdb-capstone/imdb-reviews'
   imdb_titles = 's3://imdb-capstone/imdb-datasets/title.basics.tsv'
   imdb_principals = 's3://imdb-capstone/imdb-datasets/title.principals.tsv'
   imdb_names = 's3://imdb-capstone/imdb-datasets/name.basics.tsv'
   ```

2. Run the first part of `create-aws-resources.ipynb` to create the required AWS resources.

3. Run the scripts below via the terminal:

   ```bash
   python create_tables.py
   python etl.py
   ```

4. After the scripts have run, you can execute `test.ipynb` for testing.

5. When you are done working on the cluster, run the last part of `create-aws-resources.ipynb` to delete the AWS resources created previously.

